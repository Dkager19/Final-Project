{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b130cb2f",
   "metadata": {},
   "source": [
    "Desmond Kager\n",
    "Anne Lightbody \n",
    "ESCI 895 \n",
    "December 20 2021\n",
    "\n",
    "#### Examining river regulation’s effect on flood frequency between various climates and water resource development types\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "   River regulation is the act of controlling river water level or the variability of river flows to meet human demands. Rivers are dammed and channels are rerouted in order to fulfill these demands which vary from flood control and power generation to drinking water supplies and irrigation for agriculture. Dams have a significant impact on rivers, the two most important being limiting the movement of water and sediment downstream. Reduction of flows increases channel stability downstream and limits sediment deposition making dammed rivers geometry relics of the fluvial environment that existed before dam construction. Dams also impact the magnitude and frequency of floods by absorbing many small floods and significantly reducing large ones through reservoir storage. These significant differences between regulated and unregulated rivers impact hydrologists ability to accurately estimate flood size and frequency. Accurately predicting the magnitude and frequency of floods can help city planners build resilient infrastructure that is built in accordance with flood frequency equations that account for river regulation.The goal of this study is to see how similar sized regulated and unregulated rivers flood frequency compare between various climates and water resource development types\n",
    "\n",
    "### Site description: \n",
    "\n",
    "   To demonstrate the effect that regulation has on flood frequency, 4 stream gauges were chosen two with regulation and two without regulation. In order to analyze the effect climate has on flood frequencies two of these sites are located in South Carolina and two in Northern New Hampshire. The first stream gauge I chose is located in the western side of South Carolina on the 200 mile long Saluda River. This river is heavily regulated with dams and is located in an area with substantial industrial agriculture and high waste water release. The second gauge in South Carolina is located in the nearby Reedy River. The Reedy River is a 68 mile long river which eventually joins the Saluda river 9 miles north of Lake Greenwood. I chose the Reedy River because it doesn't have any regulation from dams unlike the Saluda River. The third stream gauge I chose is located on the Connecticut River in Northern New Hampshire. This river is regulated from the Murphy dam which stores water from the first and second Connecticut Lakes. The last stream gauge is located on the Diamond River near Wentworth NH. This river does not have significant regulation affecting streamflow. Both NH gauges are located in rural areas with limited development and human influence.\n",
    "\n",
    "   Peak annual discharge was obtained from USGS streamflow gauging sites for each of the four stream gauges located in figure 1. 15 minute discharge measurements were also obtained from the USGS streamflow gauging sites in order to construct hydrographs of the specific storm events. Hourly precipitation was acquired from NOAA’s rain gauges in order to analyze how precipitation affects the discharges at the specific stream gauges. Land classification data was obtained through the National Land Cover Database and Hydrologic soil group data was obtained  through the USDA web soil survey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f1b15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import glob \n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c9de84",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NH_sites.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b1/06l63sgs3v78dr433qh4v4000000gn/T/ipykernel_1263/1181426419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimagefile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilelist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagefile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2976\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NH_sites.png'"
     ]
    }
   ],
   "source": [
    "#%% Loads in both site maps \n",
    "filelist= ['NH_sites.png','SC_sites.png',]\n",
    "\n",
    "for imagefile in filelist:\n",
    "    im=Image.open(imagefile)\n",
    "    plt.imshow(im)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48294c8",
   "metadata": {},
   "source": [
    "### Reading in Watershed background data \n",
    "Reads in land cover data, soil hydrologic group data and prints the mean basin slope data for all the watersheds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd10eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Reads in watershed data\n",
    "filenames2 = ['Landcover_data_final.csv','Soil_Data.csv','mean_basin_slope.csv']  \n",
    "dfland= pd.read_csv(filenames2[0],header=0,index_col=0)\n",
    "\n",
    "dfsoil= pd.read_csv(filenames2[1],header=0,index_col=0)\n",
    " \n",
    "dfslope= pd.read_csv(filenames2[2],header=0,index_col=0)\n",
    "print(dfslope) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a56a08",
   "metadata": {},
   "source": [
    "### Plotting catchment areas based on Landcover and Hydrologic Soil Groups\n",
    "\n",
    "Each of the four stream gauges delineated watersheds where plotted based on land classification data from the NLCD.  Many of the individual NLCD classifications were combined into a general classification, for example the NLCD categories Evergreen forest, Deciduous forest and Mixed forest were joined to make the class forest in figure1.  Hydraulic Soil groups were also plotted for each of the catchment areas using the USDA four main groups (A, B,C and D) and three dual classes (A/D, B/D and C/D). The goal of plotting hydrologic soil types and land cover classifications was to see if these factors also influence flood frequency of a particular watershed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Plots land use \n",
    "X= np.arange(1)\n",
    "fig, ax = plt.subplots(2,2, sharey=True,figsize=(10,10))\n",
    "\n",
    "ax[0,0].bar(X + 0.00, dfland.iloc[0,0], color = 'r', width = 0.1, label='Urban ')\n",
    "ax[0,0].bar(X + 0.1, dfland.iloc[0,1], color = 'y', width = 0.1, label= 'Agriculture')\n",
    "ax[0,0].bar(X + 0.2, dfland.iloc[0,2], color = 'g', width = 0.1, label= 'Forest')\n",
    "ax[0,0].bar(X + 0.3, dfland.iloc[0,3], color = 'c', width = 0.1, label= 'Wetlands')\n",
    "ax[0,0].bar(X + 0.4, dfland.iloc[0,4], color = 'b', width = 0.1, label= 'Open Water')\n",
    "ax[0,0].bar(X + 0.5, dfland.iloc[0,5], color = 'k', width = 0.1, label= 'Other')\n",
    "ax[0,0].set_title('Saluda River South Carolina ')\n",
    "ax[0,0].xaxis.set_visible(False)\n",
    "\n",
    "ax[0,1].bar(X + 0.00, dfland.iloc[1,0], color = 'r', width = 0.1, label='Urban ')\n",
    "ax[0,1].bar(X + 0.1, dfland.iloc[1,1], color = 'y', width = 0.1, label= 'Agriculture')\n",
    "ax[0,1].bar(X + 0.2, dfland.iloc[1,2], color = 'g', width = 0.1, label= 'Forest')\n",
    "ax[0,1].bar(X + 0.3, dfland.iloc[1,3], color = 'c', width = 0.1, label= 'Wetlands')\n",
    "ax[0,1].bar(X + 0.4, dfland.iloc[1,4], color = 'b', width = 0.1, label= 'Open Water')\n",
    "ax[0,1].bar(X + 0.5, dfland.iloc[1,5], color = 'k', width = 0.1, label= 'Other')\n",
    "ax[0,1].set_title('Reedy River South Carolina ')\n",
    "ax[0,1].xaxis.set_visible(False)\n",
    "\n",
    "ax[1,0].bar(X + 0.00, dfland.iloc[2,0], color = 'r', width = 0.1, label='Urban ')\n",
    "ax[1,0].bar(X + 0.1, dfland.iloc[2,1], color = 'y', width = 0.1, label= 'Agriculture')\n",
    "ax[1,0].bar(X + 0.2, dfland.iloc[2,2], color = 'g', width = 0.1, label= 'Forest')\n",
    "ax[1,0].bar(X + 0.3, dfland.iloc[2,3], color = 'c', width = 0.1, label= 'Wetlands')\n",
    "ax[1,0].bar(X + 0.4, dfland.iloc[2,4], color = 'b', width = 0.1, label= 'Open Water')\n",
    "ax[1,0].bar(X + 0.5, dfland.iloc[2,5], color = 'k', width = 0.1, label= 'Other')\n",
    "ax[1,0].set_title('Diamond River New Hampshire ')\n",
    "ax[1,0].xaxis.set_visible(False)\n",
    "\n",
    "ax[1,1].bar(X + 0.00, dfland.iloc[3,0], color = 'r', width = 0.1, label='Urban ')\n",
    "ax[1,1].bar(X + 0.1, dfland.iloc[3,1], color = 'y', width = 0.1, label= 'Agriculture')\n",
    "ax[1,1].bar(X + 0.2, dfland.iloc[3,2], color = 'g', width = 0.1, label= 'Forest')\n",
    "ax[1,1].bar(X + 0.3, dfland.iloc[3,3], color = 'c', width = 0.1, label= 'Wetlands')\n",
    "ax[1,1].bar(X + 0.4, dfland.iloc[3,4], color = 'b', width = 0.1, label= 'Open Water')\n",
    "ax[1,1].bar(X + 0.5, dfland.iloc[3,5], color = 'k', width = 0.1, label= 'Other')\n",
    "ax[1,1].set_title('CT River New Hampshire')\n",
    "ax[1,1].xaxis.set_visible(False)\n",
    "ax[0,1].legend(loc='best')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516866e",
   "metadata": {},
   "source": [
    "Figure 1. Land use classifications of the four New Hampshire and South Carolina watersheds.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f341a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SOIL Types for watersheds \n",
    "X= np.arange(1)\n",
    "fig, ax = plt.subplots(2,2, sharey=True,figsize=(10,10))\n",
    "\n",
    "ax[0,0].bar(X + 0.00, dfsoil.iloc[0,0], color = 'r', width = 0.1, label='A- High Infiltration ')\n",
    "ax[0,0].bar(X + 0.1, dfsoil.iloc[0,1], color = 'y', width = 0.1, label= 'A/D')\n",
    "ax[0,0].bar(X + 0.2, dfsoil.iloc[0,2], color = 'g', width = 0.1, label= 'B- Moderate Infiltration')\n",
    "ax[0,0].bar(X + 0.3, dfsoil.iloc[0,3], color = 'c', width = 0.1, label= 'B/D- Medium/Slow')\n",
    "ax[0,0].bar(X + 0.4, dfsoil.iloc[0,4], color = 'b', width = 0.1, label= 'C- Slow Infiltration')\n",
    "ax[0,0].bar(X + 0.5, dfsoil.iloc[0,5], color = 'k', width = 0.1, label= 'C/D Medium/ Very Slow ')\n",
    "ax[0,0].bar(X + 0.5, dfsoil.iloc[0,6], color = 'm', width = 0.1, label= 'D Very Slow Infiltration ')\n",
    "ax[0,0].set_title('Saluda River South Carolina ')\n",
    "ax[0,0].xaxis.set_visible(False)\n",
    "\n",
    "ax[0,1].bar(X + 0.00, dfsoil.iloc[1,0], color = 'r', width = 0.1, label='A- High Infiltration ')\n",
    "ax[0,1].bar(X + 0.1, dfsoil.iloc[1,1], color = 'y', width = 0.1, label= 'A/D')\n",
    "ax[0,1].bar(X + 0.2, dfsoil.iloc[1,2], color = 'g', width = 0.1, label= 'B- Moderate Infiltration')\n",
    "ax[0,1].bar(X + 0.3, dfsoil.iloc[1,3], color = 'c', width = 0.1, label= 'B/D- Medium/Slow')\n",
    "ax[0,1].bar(X + 0.4, dfsoil.iloc[1,4], color = 'b', width = 0.1, label= 'C- Slow Infiltration')\n",
    "ax[0,1].bar(X + 0.5, dfsoil.iloc[1,5], color = 'k', width = 0.1, label= 'C/D Medium/ Very Slow ')\n",
    "ax[0,1].bar(X + 0.6, dfsoil.iloc[1,6], color = 'm', width = 0.1, label= 'D Very Slow Infiltration ')\n",
    "ax[0,1].set_title('Reedy River South Carolina ')\n",
    "ax[0,1].xaxis.set_visible(False)\n",
    "\n",
    "ax[1,0].bar(X + 0.00, dfsoil.iloc[2,0], color = 'r', width = 0.1, label='A- High Infiltration ')\n",
    "ax[1,0].bar(X + 0.1, dfsoil.iloc[2,1], color = 'y', width = 0.1, label= 'A/D')\n",
    "ax[1,0].bar(X + 0.2, dfsoil.iloc[2,2], color = 'g', width = 0.1, label= 'B- Moderate Infiltration')\n",
    "ax[1,0].bar(X + 0.3, dfsoil.iloc[2,3], color = 'c', width = 0.1, label= 'B/D- Medium/Slow')\n",
    "ax[1,0].bar(X + 0.4, dfsoil.iloc[2,4], color = 'b', width = 0.1, label= 'C- Slow Infiltration')\n",
    "ax[1,0].bar(X + 0.5, dfsoil.iloc[2,5], color = 'k', width = 0.1, label= 'C/D Medium/ Very Slow ')\n",
    "ax[1,0].bar(X + 0.6, dfsoil.iloc[2,6], color = 'm', width = 0.1, label= 'D Very Slow Infiltration ')\n",
    "ax[1,0].set_title('Diamnond River New Hampshire ')\n",
    "ax[1,0].xaxis.set_visible(False)\n",
    "\n",
    "ax[1,1].bar(X + 0.00, dfsoil.iloc[3,0], color = 'r', width = 0.1, label='A- High Infiltration ')\n",
    "ax[1,1].bar(X + 0.1, dfsoil.iloc[3,1], color = 'y', width = 0.1, label= 'A/D')\n",
    "ax[1,1].bar(X + 0.2, dfsoil.iloc[3,2], color = 'g', width = 0.1, label= 'B- Moderate Infiltration')\n",
    "ax[1,1].bar(X + 0.3, dfsoil.iloc[3,3], color = 'c', width = 0.1, label= 'B/D- Medium/Slow')\n",
    "ax[1,1].bar(X + 0.4, dfsoil.iloc[3,4], color = 'b', width = 0.1, label= 'C- Slow Infiltration')\n",
    "ax[1,1].bar(X + 0.5, dfsoil.iloc[3,5], color = 'k', width = 0.1, label= 'C/D Medium/ Very Slow ')\n",
    "ax[1,1].bar(X + 0.6, dfsoil.iloc[3,6], color = 'm', width = 0.1, label= 'D Very Slow Infiltration ')\n",
    "ax[1,1].set_title('CT River New Hampshire')\n",
    "ax[1,1].xaxis.set_visible(False)\n",
    "\n",
    "ax[1,1].legend(loc='best')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f74a7",
   "metadata": {},
   "source": [
    "Figure 2. Hydrologic soil groups for each of the New Hampshire and South Carolina watersheds.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef0d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates color class\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2719d",
   "metadata": {},
   "source": [
    "### Loading in Discharge Data \n",
    "\n",
    "15 minute discharge was loaded in for both the South Carolina and New Hampshire sites. The NH 15 min discharge were merged to create dfboth. While the 15 min discharge for the SC sites were merged to create dfboth_SC. These dataframes contained 3 different years of data for both regulated and unregulated gauges in there particular watershed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a726cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loads in NH 15 min regulated data 4 files \n",
    "path = 'NH_regulated15' \n",
    "all_files = glob.glob(path + \"/*.txt\")\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None,parse_dates=['20d'],delimiter= '\\t',comment='#',  header=1)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "frame=frame[['20d','14n']]\n",
    "frame= frame.rename (columns={'20d':'DATE'})\n",
    "frame=frame.rename(columns={'14n':'discharge_R'})\n",
    "frame=frame.set_index('DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0345cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loads in NH unregulated 15 min data \n",
    "path2 = 'NH_unregulated15' \n",
    "all_files2 = glob.glob(path2 + \"/*.txt\")\n",
    "\n",
    "li2 = []\n",
    "\n",
    "for filename in all_files2:\n",
    "    df2 = pd.read_csv(filename, parse_dates=['20d'],index_col=None,delimiter= '\\t',comment='#',  header=1)\n",
    "    li2.append(df2)\n",
    "    \n",
    "frame2 = pd.concat(li2, axis=0, ignore_index=True)\n",
    "frame2=frame2[['20d','14n']]\n",
    "frame2= frame2.rename (columns={'20d':'DATE'})\n",
    "frame2=frame2.rename(columns={'14n':'discharge_U'})\n",
    "frame2=frame2.set_index('DATE')\n",
    "#Merges the data for NH regulated and Unregulated into a singular df\n",
    "\n",
    "dfboth= pd.merge(frame,frame2, on='DATE', how='outer')\n",
    "dfboth.sort_index(inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb47670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loading South Carolina regulated 15 min discharge \n",
    "path3 = 'SC_regulated15' \n",
    "all_files3 = glob.glob(path3 + \"/*.txt\")\n",
    "\n",
    "li3 = []\n",
    "for filename in all_files3:\n",
    "    df3 = pd.read_csv(filename, index_col=None,parse_dates=['20d'],delimiter= '\\t',comment='#',  header=1)\n",
    "    li3.append(df3)\n",
    "\n",
    "frame3 = pd.concat(li3, axis=0, ignore_index=True)\n",
    "\n",
    "frame3=frame3[['20d','14n']]\n",
    "frame3= frame3.rename (columns={'20d':'DATE'})\n",
    "frame3=frame3.rename(columns={'14n':'discharge_R'})\n",
    "frame3=frame3.set_index('DATE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loading South Carolina unregulated 15 min discharge \n",
    "path4 = 'SC_unregulated15' \n",
    "all_files4 = glob.glob(path4 + \"/*.txt\")\n",
    "\n",
    "li4 = []\n",
    "for filename in all_files4:\n",
    "    df4 = pd.read_csv(filename, index_col=None,parse_dates=['20d'],delimiter= '\\t',comment='#',  header=1)\n",
    "    li4.append(df4)\n",
    "    \n",
    "frame4= pd.concat(li4, axis=0, ignore_index=True)\n",
    "frame4=frame4[['20d','14n']]\n",
    "frame4= frame4.rename (columns={'20d':'DATE'})\n",
    "frame4=frame4.rename(columns={'14n':'discharge_U'})\n",
    "frame4=frame4.set_index('DATE')\n",
    "\n",
    "# Merges the data for SC regulated and Unregulated into a singular df\n",
    "dfboth_SC= pd.merge(frame3,frame4, on='DATE', how='outer')\n",
    "\n",
    "dfboth_SC.sort_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e955d",
   "metadata": {},
   "source": [
    "### Loading in Precipitation data \n",
    "Both NH and SC precipitation data were read into there respictive dataframes precip_NH and precip_SC. Trace colems where set as NAN's for both sites. Precipitation was linearly interpolated for both sites and hourly precipitation was converted from inches into centimeters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631f1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loading IN NH Precip data \n",
    "path5 = 'Precip_NH' \n",
    "all_files5 = glob.glob(path5 + \"/*.csv\")\n",
    "li5 = []\n",
    "\n",
    "for filename in all_files5:\n",
    "    df5 = pd.read_csv(filename, index_col=None,parse_dates=['DATE'],delimiter= ',',comment='#',\n",
    "                      header=0, na_values=(-9999,'T'))\n",
    "    li5.append(df5)\n",
    "\n",
    "precip_NH = pd.concat(li5, axis=0, ignore_index=True)\n",
    "precip_NH=precip_NH[['DATE','HourlyPrecipitation']]\n",
    "precip_NH=precip_NH.set_index('DATE')\n",
    "precip_NH.sort_index(inplace=True)\n",
    "\n",
    "for col in precip_NH:\n",
    "    precip_NH[col] = pd.to_numeric(precip_NH[col], errors='coerce')\n",
    "\n",
    "precip_NH.interpolate(method='linear',inplace = True)\n",
    "precip_NH.fillna(method = 'ffill',inplace=True)\n",
    "precip_NH.fillna(method = 'bfill',inplace=True)\n",
    "\n",
    "precip_NH['precipcm']=precip_NH['HourlyPrecipitation']*2.54\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Loading IN SC Precip data \n",
    "path6 = 'Precip_SC' \n",
    "all_files6 = glob.glob(path6+ \"/*.csv\")\n",
    "li6 = []\n",
    "\n",
    "for filename in all_files6:\n",
    "    df6 = pd.read_csv(filename, index_col=None,parse_dates=['DATE'],delimiter= ',',comment='#',\n",
    "                      header=0, na_values=(-9999,'T'))\n",
    "    li6.append(df6)\n",
    "\n",
    "precip_SC = pd.concat(li6, axis=0, ignore_index=True)\n",
    "precip_SC=precip_SC[['DATE','HourlyPrecipitation']]\n",
    "precip_SC=precip_SC.set_index('DATE')\n",
    "precip_SC.sort_index(inplace=True)\n",
    "\n",
    "for col in precip_SC:\n",
    "    precip_SC[col] = pd.to_numeric(precip_SC[col], errors='coerce')\n",
    "\n",
    "\n",
    "precip_SC.interpolate(method='linear',inplace = True)\n",
    "precip_SC.fillna(method = 'ffill',inplace=True)\n",
    "precip_SC.fillna(method = 'bfill',inplace=True)\n",
    "#conver inches to cm\n",
    "\n",
    "precip_SC['precipcm']=precip_SC['HourlyPrecipitation']*2.54\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c633a2",
   "metadata": {},
   "source": [
    "### Copying and Manipulating dataframes \n",
    "Before dataframes are trimmed by storm dates and normalized based on time step and watershed area copies are made for both discharge and precipitation dataframes in all sites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c156376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Creates copys of dataframes to be used for analysiss \n",
    "\n",
    "#creates copy of NH and SC precipitation\n",
    "precip_NH2= precip_NH.copy()\n",
    "precip_NH3= precip_NH.copy()\n",
    "precip_SC2= precip_SC.copy()\n",
    "precip_SC3= precip_SC.copy()\n",
    "\n",
    "#Creates copys of NH and SC discharge \n",
    "dfboth2= dfboth.copy()\n",
    "dfboth3= dfboth.copy()\n",
    "dfboth_SC2= dfboth_SC.copy()\n",
    "dfboth_SC3= dfboth_SC.copy()\n",
    "\n",
    "#Creates copy for single storm analysis \n",
    "dfq=dfboth.copy()\n",
    "dfq3= dfboth.copy()\n",
    "dfp=precip_NH.copy()\n",
    "#south carolina precip\n",
    "dfp4= precip_SC.copy()\n",
    "#south carolina unregulated\n",
    "dfq5= dfboth_SC.copy()\n",
    "#regulated sc\n",
    "dfq6= dfboth_SC.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8bf99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% List containin names of  daily discharge and annual peak flow\n",
    "\n",
    "filenames = ['peak_regulatedNH.txt','peak_unregulatedNH.txt','peakunregulatedSC.txt','peakregulatedSC.txt']  \n",
    "dfpeak= pd.read_csv(filenames[0], delimiter= '\\t',comment= '#',header=1,\n",
    "                 parse_dates=['10d'])\n",
    "dfpeak2= pd.read_csv(filenames[1], delimiter= '\\t',comment= '#',header=1,\n",
    "                 parse_dates=['10d'])\n",
    "dfpeak3= pd.read_csv(filenames[2], delimiter= '\\t',comment= '#',header=1,\n",
    "                 parse_dates=['10d'])\n",
    "dfpeak4= pd.read_csv(filenames[3], delimiter= '\\t',comment= '#',header=1,\n",
    "                 parse_dates=['10d'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5dc410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sorrting peak data \n",
    "#NH unregulated dfpeak\n",
    "dfpeak=dfpeak.rename(columns={'10d': 'DATE'})\n",
    "dfpeak=dfpeak.rename(columns={'8s': 'peak_flow'})\n",
    "dfpeak= dfpeak.set_index('DATE')\n",
    "dfpeak= dfpeak[['peak_flow']]\n",
    "\n",
    "#NH dfpeak regulated\n",
    "dfpeak2=dfpeak2.rename(columns={'10d': 'DATE'})\n",
    "dfpeak2=dfpeak2.rename(columns={'8s': 'peak_flow'})\n",
    "dfpeak2= dfpeak2.set_index('DATE')\n",
    "dfpeak2= dfpeak2[['peak_flow']]\n",
    "\n",
    "#SC unregulated dfpeaknflow\n",
    "dfpeak3=dfpeak3.rename(columns={'10d': 'DATE'})\n",
    "dfpeak3=dfpeak3.rename(columns={'8s': 'peak_flow'})\n",
    "dfpeak3= dfpeak3.set_index('DATE')\n",
    "dfpeak3= dfpeak3[['peak_flow']]\n",
    "dfpeak3= dfpeak3.drop_duplicates()\n",
    "\n",
    "#SC dfpeak regulated\n",
    "dfpeak4=dfpeak4.rename(columns={'10d': 'DATE'})\n",
    "dfpeak4=dfpeak4.rename(columns={'8s': 'peak_flow'})\n",
    "dfpeak4= dfpeak4.set_index('DATE')\n",
    "dfpeak4= dfpeak4[['peak_flow']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e6d18",
   "metadata": {},
   "source": [
    "\n",
    "### Sorting and converting peak annual discharges for stream gauges \n",
    "\n",
    "Each of the four stream gauges' annual peak discharge data was converted to  areal depth in order to account for the differences in watershed sizes. This was accomplished by dividing peak flows by the watershed area and appropriate time step in order to get units of cm/year.  The mean and standard deviation of these flows were taken in order to inspect the initial differences between the watersheds peak discharges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fa14d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% converting discharge as an aeral depth by dividing by watershed area\n",
    "watershedarea=  254*2589988.11\n",
    "\n",
    "watershedarea2=  153*2589988.11\n",
    "\n",
    "watershedarea3=  240*2589988.11\n",
    "\n",
    "watershedarea4=  580*2589988.11\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfpeak[\"peak_flow\"] = (dfpeak['peak_flow']*28316*3600*24*365)/(watershedarea*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfpeak2[\"peak_flow\"] = (dfpeak2['peak_flow']*28316*3600*24*365)/(watershedarea2*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfpeak3[\"peak_flow\"] = (dfpeak3['peak_flow']*28316*3600*24*365)/(watershedarea3*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfpeak4[\"peak_flow\"] = (dfpeak4['peak_flow']*28316*3600*24*365)/(watershedarea4*10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d798be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% mean peaks accounting for area\n",
    "\n",
    "def average(peak):\n",
    "   \n",
    "    mean= peak['peak_flow'].mean()\n",
    "    print(mean)\n",
    "    standard_deviation= peak['peak_flow'].std()\n",
    "    print(standard_deviation)\n",
    "  \n",
    "    return peak\n",
    "print(color.BOLD +\"NH average regulated discharge and std  \"+ color.END)\n",
    "average(dfpeak)\n",
    "print(color.BOLD +\"NH average unregulated discharge and std  \"+ color.END)\n",
    "average(dfpeak2)\n",
    "print(color.BOLD +\"SC average unregulated discharge and std  \"+ color.END)\n",
    "average(dfpeak3)\n",
    "print(color.BOLD +\"SC average regulated discharge and std  \"+ color.END)\n",
    "average(dfpeak4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Trimming Peak data\n",
    "startdate1= dt.datetime(1957,1,1)\n",
    "enddate1= dt.datetime(2019,1,1)\n",
    "\n",
    "startdate2= dt.datetime(1940,1,1)\n",
    "enddate2= dt.datetime(2002,1,1)\n",
    "\n",
    "dfpeak= dfpeak[startdate1:enddate1]\n",
    "dfpeak2= dfpeak2[startdate1:enddate1]\n",
    "dfpeak3= dfpeak3[startdate2:enddate2]\n",
    "dfpeak4= dfpeak4[startdate2:enddate2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3140d5e8",
   "metadata": {},
   "source": [
    "### Estimating annual flood probabilities between watersheds (Weibull)\n",
    "\n",
    "Peak annual flows were sorted for each of the watersheds in descending order. The Weibull plotting position formula(EP= R/N+1)  was used to calculate the Exceedance probabilities for each of the stream gauges peak flows, where EP is the exceedance probability and R is the rank and N the number of measurements.  The return interval was calculated for each of the stream gauges with the equation (TR= 1/EP), the return interval associated with a particular exceedance probability is its reciprocal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7252866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Sortting daily peak by peak flow\n",
    "sorted_daily_peak= dfpeak.sort_values(by= 'peak_flow',   ascending= False)\n",
    "sorted_daily_peak2= dfpeak2.sort_values(by= 'peak_flow', ascending= False)\n",
    "sorted_daily_peak3= dfpeak3.sort_values(by= 'peak_flow', ascending= False)\n",
    "sorted_daily_peak4= dfpeak4.sort_values(by= 'peak_flow', ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae194f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% return period function\n",
    "def return_period(sorted_peak):\n",
    "\n",
    "    \n",
    "    #create new column rank to determine the highest flows \n",
    "    sorted_peak['rank']= sorted_peak['peak_flow'].rank( ascending= False, method= 'first')\n",
    "    #number of years is equal to n\n",
    "    n= len(sorted_peak)\n",
    "    #calculate the exceedence probablity \n",
    "    sorted_peak['EP']= sorted_peak['rank']/(n+1) \n",
    "    #caluclates the return interval\n",
    "    sorted_peak['TR']= (1/ sorted_peak['EP'])\n",
    "    \n",
    "    return sorted_peak\n",
    "\n",
    "return_period(sorted_daily_peak)\n",
    "return_period(sorted_daily_peak2)\n",
    "return_period(sorted_daily_peak3)\n",
    "return_period(sorted_daily_peak4)\n",
    "\n",
    "\n",
    "sorted_daily_values= sorted_daily_peak.sort_values(by= 'TR'   )\n",
    "sorted_daily_values2= sorted_daily_peak2.sort_values(by= 'TR'   )\n",
    "sorted_daily_values3= sorted_daily_peak3.sort_values(by= 'TR'   )\n",
    "sorted_daily_values4= sorted_daily_peak4.sort_values(by= 'TR'   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc685588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% rank function \n",
    "                                            \n",
    "interp= np.array([2,5,10,25,50,100])\n",
    "\n",
    "dfinterp= pd.DataFrame(interp, columns= ['Return Period'])\n",
    "dfinterp2= pd.DataFrame(interp, columns= ['Return Period'])\n",
    "dfinterp3= pd.DataFrame(interp, columns= ['Return Period'])\n",
    "dfinterp4= pd.DataFrame(interp, columns= ['Return Period'])\n",
    "\n",
    "def rank(sorted_peak):\n",
    "    \n",
    "    sorted_peak= sorted_peak.sort_values(by= 'rank', ascending= False)\n",
    "    return sorted_peak\n",
    "\n",
    "rank(sorted_daily_peak)\n",
    "rank(sorted_daily_peak2)\n",
    "rank(sorted_daily_peak3)\n",
    "rank(sorted_daily_peak4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4fa98",
   "metadata": {},
   "source": [
    "### Estimating annual flood probabilities between watersheds (Log Pearson III )\n",
    "\n",
    "The log Pearson III was also used to predict annual flood probabilities by using the previously log transformed mean, standard deviation and the estimated skew.  The equation is the mean + standard deviation * KgEP(frequency factor).  Skew of 0 was used for South Carolina and .3 for New Hampshire based on the USGS generalized skew coefficients map. The log was reversed to obtain discharge measurements The log pearson III is useful because it can be used to estimate floods well beyond the observed flood events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a8cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Interp function \n",
    "\n",
    "def interp(df_interp,sorted_peak,peak):\n",
    "    \n",
    "    df_interp['interp_values']= np.interp(df_interp['Return Period'], sorted_peak['TR'],\n",
    "                                       sorted_peak['peak_flow'])\n",
    "    #log transformed data  3.1\n",
    "    peak['log10']= np.log10(peak['peak_flow'])\n",
    "    # 3.3 log means \n",
    "    mean_log= peak['log10'].mean()\n",
    "    print('mean log is' +str(mean_log))\n",
    "    #calculates standard deviation of peak flow log\n",
    "    standard_deviation_log= peak['log10'].std()\n",
    "    print('standard deviation log is' +str(standard_deviation_log))\n",
    "    #adds the EP to the dfinterp column \n",
    "    df_interp['EP']= 1/df_interp['Return Period']\n",
    "    #calculates the log normal frequncy factor and add kep to interp\n",
    "    df_interp['Kep']= ((1-df_interp['EP'])**.135-(df_interp['EP'])**.135)/.1975\n",
    "\n",
    "    #equation 4 log discharge and add to dfinterp\n",
    "    df_interp['logQp']= peak['log10'].mean() +peak['log10'].std()*df_interp['Kep']\n",
    "\n",
    "    #calculated for the dfinterp dataframe\n",
    "    df_interp['est discharge logQp']=10**(df_interp['logQp'])\n",
    "    \n",
    "    return df_interp\n",
    "print('NH Regulated')\n",
    "interp(dfinterp,sorted_daily_values,dfpeak)\n",
    "print('NH Unregulated')\n",
    "interp(dfinterp2,sorted_daily_values2,dfpeak2)\n",
    "print('SC Unregulated')\n",
    "interp(dfinterp3,sorted_daily_values3,dfpeak3)\n",
    "print(' SC Regulated')\n",
    "interp(dfinterp4,sorted_daily_values4,dfpeak4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% GSX function \n",
    "def gsx(interp,peak):\n",
    "    \n",
    "    # section 4.2 gsx equation \n",
    "    N=len(interp)\n",
    "    gsx= (N*sum((peak['log10']\n",
    "                            -peak['log10'].mean())**3)/((N-1)*(N-2)*peak['log10'].mean()**3))\n",
    "    print('gsx='+ str(gsx))\n",
    "    \n",
    "    #calculate the  mean square error of station skew\n",
    "    print('regional skew is .3')\n",
    "    #based on USGS regional skew map \n",
    "    grx= .3\n",
    "    #equations based on gsx values for mean square error \n",
    "    a= -.33 +.08+gsx\n",
    "    b= .94 -.26 +gsx\n",
    "    \n",
    "    MSEgsx= (10**(a+b)/N**b)\n",
    "    print('MSEgsx='+ str(MSEgsx))\n",
    "    #given mean square error regional skew \n",
    "    MSEgrx= .302\n",
    "    #the minimum mean square error weighted skew estimate\n",
    "    gx= ((gsx/MSEgsx)+(grx/MSEgrx))/((1/MSEgsx)+(1/MSEgrx))\n",
    "    print('gx=' +str(gx))\n",
    "    #frequency factor kg exceedence probablity\n",
    "    interp['Kg']= (2/gx)*(1+gx*((((1-interp['EP'])**.135)-\n",
    "                      interp['EP']**.135)/1.185) - ((gx/36)**2))**3 - (2/gx)\n",
    "    #equation 5 log discharge and add to dfinterp\n",
    "    interp['lognew']= peak['log10'].mean() +peak['log10'].std()*interp['Kg']\n",
    "    \n",
    "    #calculated for the dfinterp dataframe\n",
    "    interp['lognew']=10**(interp['lognew'])\n",
    "    \n",
    "    return interp\n",
    "\n",
    "print(color.BOLD +\"NH Regulated   \"+ color.END)\n",
    "gsx(dfinterp,dfpeak)\n",
    "print(color.BOLD +\"NH Unregulated   \"+ color.END)\n",
    "gsx(dfinterp2,dfpeak2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50514397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% GSX2 function \n",
    "def gsx2(interp,peak):\n",
    "    \n",
    "    # section 4.2 gsx equation \n",
    "    N=len(interp)\n",
    "    gsx= (N*sum((peak['log10']\n",
    "                            -peak['log10'].mean())**3)/((N-1)*(N-2)*peak['log10'].mean()**3))\n",
    "    print('gsx='+ str(gsx))\n",
    "    \n",
    "    #calculate the  mean square error of station skew\n",
    "    print('regional skew is 0')\n",
    "    #based on USGS regional skew map \n",
    "    grx= 0\n",
    "    #equations based on gsx values for mean square error \n",
    "    a= -.33 +.08+gsx\n",
    "    b= .94 -.26 +gsx\n",
    "    \n",
    "    MSEgsx= (10**(a+b)/N**b)\n",
    "    print('MSEgsx='+ str(MSEgsx))\n",
    "    #given mean square error regional skew \n",
    "    MSEgrx= .302\n",
    "    #the minimum mean square error weighted skew estimate\n",
    "    gx= ((gsx/MSEgsx)+(grx/MSEgrx))/((1/MSEgsx)+(1/MSEgrx))\n",
    "    print('gx=' +str(gx))\n",
    "    #frequency factor kg exceedence probablity\n",
    "    interp['Kg']= (2/gx)*(1+gx*((((1-interp['EP'])**.135)-\n",
    "                      interp['EP']**.135)/1.185) - ((gx/36)**2))**3 - (2/gx)\n",
    "    #equation 5 log discharge and add to dfinterp\n",
    "    interp['lognew']= peak['log10'].mean() +peak['log10'].std()*interp['Kg']\n",
    "    \n",
    "    #calculated for the dfinterp dataframe\n",
    "    interp['lognew']=10**(interp['lognew'])\n",
    "    \n",
    "    return interp\n",
    "\n",
    "print(color.BOLD +\"SC Unregulated   \"+ color.END)\n",
    "gsx2(dfinterp3,dfpeak3)\n",
    "print(color.BOLD +\"SC Regulated   \"+ color.END)\n",
    "gsx2(dfinterp4,dfpeak4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8edc56",
   "metadata": {},
   "source": [
    "### Plotting annual peak discharge vs return periods \n",
    "Both the NH and SC return intervals were plotted against the sorted annual peak discharges. The log Pearson III extrapolations where plotted as well with dfinterp return periods being plotted against the lognew thus creating the two figures bellow (figure 3 and 4).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% NH Plotting \n",
    "fig,(ax4)= plt.subplots(1,1)\n",
    "ax4.set_title('NH Regulated and unregulated Log Pearson III  ')\n",
    "#plots the measured peak discharge values \n",
    "ax4.plot(sorted_daily_peak['TR'],sorted_daily_peak['peak_flow'],'o', color= 'k')\n",
    "                    \n",
    "ax4.set_xlabel('Return Period years')\n",
    "#plot log pearson 3\n",
    "ax4.plot(dfinterp['Return Period'], dfinterp['lognew'], color= 'k', label= 'Regulated')\n",
    "ax4.legend(loc='lower center')\n",
    "#plots the measured peak discharge values \n",
    "ax4.plot(sorted_daily_peak2['TR'],sorted_daily_peak2['peak_flow'],'o', color= 'r')\n",
    "ax4.set_xlabel('Return Period years')\n",
    "\n",
    "ax4.plot(dfinterp2['Return Period'], dfinterp2['lognew'], color= 'r', label= 'Unregulated')\n",
    "\n",
    "ax4.legend(loc='lower center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4ba514",
   "metadata": {},
   "source": [
    "Figure 3. New Hampshire regulated and unregulated annual peak discharge vs return periods with trend lines depiciting log pearson III extrapolations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4452d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% SC plotting \n",
    "fig,(ax5)= plt.subplots(1,1)\n",
    "ax5.set_title('SC Regulated and unregulated Log Pearson III  ')\n",
    "ax5.plot(sorted_daily_peak3['TR'],sorted_daily_peak3['peak_flow'],'o', color= 'r')\n",
    "ax5.set_xlabel('Return Period years')\n",
    "\n",
    "ax5.plot(dfinterp3['Return Period'], dfinterp3['lognew'], color= 'r', label= 'Unregulated')\n",
    "#plots the measured peak discharge values \n",
    "ax5.plot(sorted_daily_peak4['TR'],sorted_daily_peak4['peak_flow'],'o', color= 'k')\n",
    "                    \n",
    "ax5.set_xlabel('Return Period years')\n",
    "\n",
    "#plot log pearson 3\n",
    "ax5.plot(dfinterp4['Return Period'], dfinterp4['lognew'], color= 'k', label= 'Regulated')\n",
    "ax5.legend(loc='lower center')\n",
    "\n",
    "ax5.legend(loc='lower center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4512b14",
   "metadata": {},
   "source": [
    "Figure 4. South Carolina regulated and unregulated annual peak discharge vs return periods with trend lines depicting log pearson III extrapolations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71856f3f",
   "metadata": {},
   "source": [
    "\n",
    "### Plotting storm events \n",
    "\n",
    "Six storm events were chosen, three for the NH gauges and three for the SC gauges. \n",
    "Regulated and unregulated precipitation were plotted with a shared inverted y axis containing precipitation data. These plots provided a visual of how the rivers reacted to precipitation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d3f32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Dates for each storm\n",
    "\n",
    "#need it to run through each storm \n",
    "startdate_1= (dt.datetime(2019,10,7))\n",
    "enddate_1= (dt.datetime(2019,10,10))\n",
    "\n",
    "startdate_2= (dt.datetime(2011,5,26))\n",
    "enddate_2= (dt.datetime(2011,5,30))\n",
    "\n",
    "startdate_3= (dt.datetime(2005,10,14))\n",
    "enddate_3= (dt.datetime(2005,10,19))\n",
    "\n",
    "startdate_4= (dt.datetime(1990,10,22))\n",
    "enddate_4= (dt.datetime(1990,10,27))\n",
    "\n",
    "startdate_5= (dt.datetime(1998,3,7))\n",
    "enddate_5= (dt.datetime(1998,3,14))\n",
    "\n",
    "startdate_6= (dt.datetime(1995,8,26))\n",
    "enddate_6= (dt.datetime(1995,9,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9e84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert discharge data from ft^3/s to cm/hr regulated SC\n",
    "dfboth_SC[\"discharge_R\"] = (dfboth_SC['discharge_R']*28316*3600)/(watershedarea4*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated SC\n",
    "dfboth_SC[\"discharge_U\"] = (dfboth_SC['discharge_U']*28316*3600)/(watershedarea3*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated SC\n",
    "dfboth_SC2[\"discharge_R\"] = (dfboth_SC2['discharge_R']*28316*3600)/(watershedarea4*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated SC\n",
    "dfboth_SC2[\"discharge_U\"] = (dfboth_SC2['discharge_U']*28316*3600)/(watershedarea3*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated SC\n",
    "dfboth_SC3[\"discharge_R\"] = (dfboth_SC3['discharge_R']*28316*3600)/(watershedarea4*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated SC\n",
    "dfboth_SC3[\"discharge_U\"] = (dfboth_SC3['discharge_U']*28316*3600)/(watershedarea3*10000)\n",
    "\n",
    "\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfboth[\"discharge_R\"] = (dfboth['discharge_R']*28316*3600)/(watershedarea*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfboth[\"discharge_U\"] = (dfboth['discharge_U']*28316*3600)/(watershedarea2*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfboth2[\"discharge_R\"] = (dfboth2['discharge_R']*28316*3600)/(watershedarea*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfboth2[\"discharge_U\"] = (dfboth2['discharge_U']*28316*3600)/(watershedarea2*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfboth3[\"discharge_R\"] = (dfboth3['discharge_R']*28316*3600)/(watershedarea*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfboth3[\"discharge_U\"] = (dfboth3['discharge_U']*28316*3600)/(watershedarea2*10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ee25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Time series plot for 6 storms\n",
    "\n",
    "\n",
    "#creates plotting function that does plotting for all  NH timeseries \n",
    "def timeseriesplot(dfboth,dfboth2,dfboth3,dfboth_SC,dfboth_SC2,dfboth_SC3,\n",
    "                   precip_NH,precip_NH2,precip_NH3,precip_SC,precip_SC2,precip_SC3,\n",
    "                   startdate_1,enddate_1,startdate_2,enddate_2,startdate_3,enddate_3,\n",
    "                   startdate_4,enddate_4,startdate_5,enddate_5,startdate_6,enddate_6):\n",
    "    \n",
    "    fig, ax = plt.subplots(2,3, figsize=(10,10))\n",
    "    \n",
    "    ax[0,0].set_ylim(bottom= 0, top=4)\n",
    "    ax[0,0].invert_yaxis()\n",
    "    ax[0,0]= ax[0,0].twinx()\n",
    "    \n",
    "    ax[0,1].set_ylim(bottom= 0, top=4)\n",
    "    ax[0,1].invert_yaxis()\n",
    "    ax[0,1]= ax[0,1].twinx()\n",
    "    \n",
    "    ax[0,2].set_ylim(bottom= 0, top=4)\n",
    "    ax[0,2].invert_yaxis()\n",
    "    ax[0,2]= ax[0,2].twinx()\n",
    "    #ax[0,0].set_ylabel('Discharge (mm/day)', color='r') \n",
    "                           \n",
    "    ax[0,0].plot(precip_NH['precipcm'], color = 'b', linestyle = '-', label = 'precip')\n",
    "    #ax[0,0].set_title('RegulatedNH')\n",
    "    \n",
    "    ax[0,1].plot(precip_NH2['precipcm'], color = 'b', linestyle = '-', label = 'precip')\n",
    "    #ax[0,1].set_title('RegulatedNH')\n",
    "    \n",
    "    ax[0,2].plot(precip_NH3['precipcm'], color = 'b', linestyle = '-', label = 'precip')\n",
    "    #ax[0,2].set_title('RegulatedNH')\n",
    "    \n",
    "    ax[0,0].set_ylim(bottom= 0, top=4)\n",
    "    ax[0,0].invert_yaxis()\n",
    "    ax[0,0]= ax[0,0].twinx()\n",
    "    \n",
    "    ax[0,1].set_ylim(bottom= 0, top=4)\n",
    "    ax[0,1].invert_yaxis()\n",
    "    ax[0,1]= ax[0,1].twinx()\n",
    "    \n",
    "    \n",
    "    ax[0,2].set_ylim(bottom= 0, top=4)\n",
    "    ax[0,2].invert_yaxis()\n",
    "    ax[0,2]= ax[0,2].twinx()\n",
    "\n",
    "    ax[0,0].plot(dfboth['discharge_R'], color = 'black', linestyle = '-', label = 'discharge')\n",
    "   # ax[0,0].set_title('RegulatedNH')\n",
    "    \n",
    "    ax[0,0].plot(dfboth['discharge_U'], color = 'red', linestyle = '-', label = 'discharge')\n",
    "    #ax[0,0].set_title('UnregulatedNH')\n",
    "    \n",
    "    ax[0,1].plot(dfboth2['discharge_R'], color = 'black', linestyle = '-', label = 'discharge')\n",
    "    ax[0,1].set_title('Regulated and Unregulated NH')\n",
    "   \n",
    "    ax[0,1].plot(dfboth2['discharge_U'], color = 'red', linestyle = '-', label = 'discharge')\n",
    "    #ax[0,1].set_title('UnregulatedNH')\n",
    "    \n",
    "    ax[0,2].plot(dfboth3['discharge_R'], color = 'black', linestyle = '-', label = 'discharge')\n",
    "    #ax[0,2].set_title('RegulatedNH')\n",
    " \n",
    "    ax[0,2].plot(dfboth3['discharge_U'], color = 'red', linestyle = '-', label = 'discharge')\n",
    "    \n",
    "    #South Carolina PLots\n",
    "    ax[1,0].set_ylim(bottom= 0, top=5)\n",
    "    ax[1,0].invert_yaxis()\n",
    "    ax[1,0]= ax[1,0].twinx()\n",
    "   \n",
    " \n",
    "    ax[1,0].plot(precip_SC['precipcm'], color = 'blue', linestyle = '-', label = 'precip')\n",
    "    #ax[1,0].set_title('Regulated SC')\n",
    "    \n",
    "    \n",
    "    ax[1,1].plot(precip_SC2['precipcm'], color = 'blue', linestyle = '-', label = 'precip')\n",
    "    #ax[1,1].set_title('Unregulated SC')\n",
    "    \n",
    "     \n",
    "    ax[1,2].plot(precip_SC3['precipcm'], color = 'blue', linestyle = '-', label = 'precip')\n",
    "    #ax[1,2].set_title('Unregulated SC')\n",
    "    \n",
    "    ax[1,0].set_ylim(bottom= 0, top=5)\n",
    "    ax[1,0].invert_yaxis()\n",
    "    ax[1,0]= ax[1,0].twinx()\n",
    "     \n",
    "    ax[1,1].set_ylim(bottom= 0, top=5)\n",
    "    ax[1,1].invert_yaxis()\n",
    "    ax[1,1]= ax[1,1].twinx()\n",
    " \n",
    "\n",
    "    ax[1,2].set_ylim(bottom= 0, top=5)\n",
    "    ax[1,2].invert_yaxis()\n",
    "    ax[1,2]= ax[1,2].twinx()\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax[1,0].plot(dfboth_SC['discharge_R'], color = 'black', linestyle = '-', label = 'discharge')\n",
    "    #ax[1,0].set_title('Regulated SC')\n",
    "    \n",
    "    ax[1,0].plot(dfboth_SC['discharge_U'], color = 'red', linestyle = '-', label = 'discharge')\n",
    "    #ax[1,0].set_title('Regulated SC')\n",
    "\n",
    "    ax[1,1].plot(dfboth_SC2['discharge_R'], color = 'black', linestyle = '-', label = 'discharge')\n",
    "    ax[1,1].set_title('Regulated and Unregulated SC')\n",
    "    \n",
    "    ax[1,1].plot(dfboth_SC2['discharge_U'], color = 'red', linestyle = '-', label = 'discharge')\n",
    "    #ax[1,1].set_title('Regulated SC')\n",
    "    \n",
    "    ax[1,2].plot(dfboth_SC3['discharge_R'], color = 'black', linestyle = '-', label = 'discharge')\n",
    "    #ax[1,2].set_title('Unregulated SC')\n",
    "    \n",
    "    ax[1,2].plot(dfboth_SC3['discharge_U'], color = 'red', linestyle = '-', label = 'discharge')\n",
    "    \n",
    "    #ax[1,2].set_title('Regulated SC')\n",
    "    ax[0,0].yaxis.set_visible(False)\n",
    "    ax[0,1].yaxis.set_visible(False)\n",
    "    ax[0,1].xaxis.set_visible(False)\n",
    "    \n",
    "    ax[1,0].yaxis.set_visible(False)\n",
    "    ax[1,1].yaxis.set_visible(False)\n",
    "    ax[1,1].xaxis.set_visible(False)\n",
    "    \n",
    "    \n",
    "    #ax[0,2].yaxis.set_visible(False)\n",
    "\n",
    "    ax[0,0].set_xlim(startdate_1,enddate_1)\n",
    "    ax[0,1].set_xlim(startdate_2,enddate_2)\n",
    "    ax[0,2].set_xlim(startdate_3,enddate_3)\n",
    "    #fig.autofmt_xdate()\n",
    "    ax[1,0].set_xlim(startdate_4,enddate_4)\n",
    "    #fig.autofmt_xdate()\n",
    "    ax[1,1].set_xlim(startdate_5,enddate_5)\n",
    "    #fig.autofmt_xdate()\n",
    "    ax[1,2].set_xlim(startdate_6,enddate_6)\n",
    "    \n",
    "    fig.autofmt_xdate()\n",
    "timeseriesplot(dfboth,dfboth2,dfboth3,dfboth_SC,dfboth_SC2,dfboth_SC3,\n",
    "               precip_NH,precip_NH2,precip_NH3,precip_SC,precip_SC2,precip_SC3,\n",
    "               startdate_1,enddate_1,startdate_2,enddate_2,startdate_3,enddate_3,\n",
    "               startdate_4,enddate_4,startdate_5,enddate_5,startdate_6,enddate_6)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7232cc",
   "metadata": {},
   "source": [
    "Figure 5. New Hampshire and South Carolina storm event hydrographs with precipitation plotted on the inverted y axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894fe11",
   "metadata": {},
   "source": [
    "### Hydrograph Separation and Analyzing storms \n",
    "\n",
    "Hydrosep function was created to calculate the antecedent discharge, event duration, ending discharge, baseflow and each of the dates associated with time of rise and fall. Another function was created to calculate each storm's precipitation, discharge antecedent discharge, maximum precipitation intensity, peak event discharge, duration of water input and centroid lag to peak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee42459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% CONVERSIONS BEFORE PLOTTING\n",
    "# Convert precip from in/hr --> cm/hr\n",
    "dfp['precip(cm/hr)'] = (dfp['HourlyPrecipitation'] * 2.54)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfq[\"discharge(cm/hr)\"] = (dfq['discharge_R']*28316*3600)/(watershedarea*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated NH\n",
    "dfq3[\"discharge(cm/hr)\"] = (dfq3['discharge_U']*28316*3600)/(watershedarea2*10000)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr unregulated SC\n",
    "dfq5[\"discharge(cm/hr)\"] = (dfq5['discharge_U']*28316*3600)/(watershedarea3*10000)\n",
    "\n",
    "# Convert precip from in/hr --> cm/hr sc\n",
    "dfp4['precip(cm/hr)'] = (dfp4['HourlyPrecipitation'] * 2.54)\n",
    "\n",
    "# Convert discharge data from ft^3/s to cm/hr regulated SC\n",
    "dfq6[\"discharge(cm/hr)\"] = (dfq6['discharge_R']*28316*3600)/(watershedarea4*10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640d163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Dates to run through \n",
    "#creates df of storms to look at\n",
    "date= {'startdates': [dt.datetime(2019,10,7),dt.datetime(2011,5,26),dt.datetime(2005,10,14)],\n",
    "       'enddates':[dt.datetime(2019,10,10),dt.datetime(2011,5,30),dt.datetime(2005,10,19)]}\n",
    "dfstorm= pd.DataFrame(data=date)\n",
    "\n",
    "#creates df of storms to look at\n",
    "date= {'startdates': [dt.datetime(2019,10,7),dt.datetime(2011,5,26),dt.datetime(2005,10,14)],\n",
    "       'enddates':[dt.datetime(2019,10,10),dt.datetime(2011,5,30),dt.datetime(2005,10,19)]}\n",
    "dfstorm2= pd.DataFrame(data=date)\n",
    "\n",
    "#creates df of storms to look at\n",
    "date= {'startdates': [dt.datetime(1990,10,22),dt.datetime(1998,3,7),dt.datetime(1995,8,26)],\n",
    "       'enddates':[dt.datetime(1990,10,26),dt.datetime(1998,3,14),dt.datetime(1995,9,1)]}\n",
    "dfstorm3= pd.DataFrame(data=date)\n",
    "\n",
    "#creates df of storms to look at\n",
    "date= {'startdates': [dt.datetime(1990,10,22),dt.datetime(1998,3,7),dt.datetime(1995,8,26)],\n",
    "       'enddates':[dt.datetime(1990,10,26),dt.datetime(1998,3,14),dt.datetime(1995,9,1)]}\n",
    "dfstorm4= pd.DataFrame(data=date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Hydrosep function \n",
    "\n",
    "def hydrosep(totalq):\n",
    "\n",
    "    #  totalq \n",
    "    totalq['diff'] = totalq['discharge(cm/hr)'].diff()\n",
    "    \n",
    "    # antecedent discharge \n",
    "    antQ = (totalq.loc[totalq['diff'] > 0.0001, 'discharge(cm/hr)'])\n",
    "    antQ_date = antQ.index[0]\n",
    "    antQ_val = round(antQ[0],3)\n",
    "    \n",
    "  \n",
    "    peakQ_date = totalq['discharge(cm/hr)'].idxmax()\n",
    "    peakQ = totalq['discharge(cm/hr)'].max()\n",
    "    \n",
    "    \n",
    "    # Calculate event duration\n",
    "    N = 0.82*(watershedarea*1e-6)**0.2\n",
    "    \n",
    "    end_of_event = peakQ_date + dt.timedelta(days = N)\n",
    "   \n",
    "    #  discharge nearest to the end of storm event\n",
    "    ending_discharge = totalq.iloc[totalq.index.get_loc(end_of_event,method='nearest'),1]\n",
    "    \n",
    "    \n",
    "    baseq = totalq[['discharge(cm/hr)']].copy()\n",
    "    \n",
    "    # line of best fit for discharge before antecedent date\n",
    "    slope1,intercept1 = np.polyfit(totalq.loc[totalq.index<antQ_date].index.view('int64')/1e9,\n",
    "                                   totalq.loc[totalq.index<antQ_date, \"discharge(cm/hr)\"],1)\n",
    "    \n",
    "    \n",
    "    #  function to estimate baseflow from antecedent date to peak date\n",
    "    baseq.loc[antQ_date:peakQ_date,\"discharge(cm/hr)\"] = slope1*(totalq.loc[antQ_date:peakQ_date].index.view('int64')/1e9) + intercept1\n",
    "    \n",
    "    # line of best fit for discharge between peak date and end of event\n",
    "    slope2,intercept2 = np.polyfit([peakQ_date.timestamp(),end_of_event.timestamp()],\n",
    "                                   [baseq.loc[peakQ_date,\"discharge(cm/hr)\"],ending_discharge],1)\n",
    "    \n",
    "    # function  estimates baseflow from peak date to end of event \n",
    "    baseq.loc[peakQ_date:end_of_event,\"discharge(cm/hr)\"] = slope2*(totalq.loc[peakQ_date:end_of_event].index.view('int64')/1e9) + intercept2\n",
    "\n",
    "    return (baseq,antQ_date,antQ_val,peakQ_date,peakQ,end_of_event,ending_discharge)\n",
    "\n",
    "# for loop for storms \n",
    "for i, v in dfstorm.iterrows():\n",
    "    \n",
    "    storm_start = dfstorm.loc[i,'startdates']\n",
    "    storm_end = dfstorm.loc[i,'enddates']\n",
    "\n",
    "    try:\n",
    "        baseq,antQ_date,antQ_val,peakQ_date,peakQ,end_of_event,ending_discharge = hydrosep(dfq[storm_start:storm_end])\n",
    "        baseq,antQ_date,antQ_val,peakQ_date,peakQ,end_of_event,ending_discharge = hydrosep(dfq3[storm_start:storm_end])\n",
    "    \n",
    "    except:\n",
    "        baseq,antQ_date,antQ_val,peakQ_date,peakQ,end_of_event,ending_discharge = hydrosep(dfq5[storm_start:storm_end])\n",
    "        baseq,antQ_date,antQ_val,peakQ_date,peakQ,end_of_event,ending_discharge = hydrosep(dfq6[storm_start:storm_end])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe12fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Analyze storm function \n",
    "\n",
    "def analyzestorm(df, dfp, dfq):\n",
    "\n",
    "    # Columns to be added to dfstorm\n",
    "    column_names = ['Total precipitation (cm)', 'Total discharge (cm)','Antecedent discharge (cm/hr)',\n",
    "                    'Maximum precipitation intensity (cm/hr)','Peak event discharge (cm/hr)', \n",
    "                    'Duration of water input (days)', 'Centroid lag to peak (days)']\n",
    "\n",
    "\n",
    "    df[column_names] = 0    \n",
    "\n",
    "    for i, v in df.iterrows():\n",
    "        \n",
    "        \n",
    "        dfp2 = dfp[df.loc[i,'startdates']:df.loc[i,'enddates']]\n",
    "        dfq2 = dfq[df.loc[i,'startdates']:df.loc[i,'enddates']]\n",
    "        \n",
    "        # Print storm number should be totla of 6 labeld 1 2 and 3 \n",
    "        print(\"\\nStorm \" + str(i+1))\n",
    "        \n",
    "        # Total precipitation \n",
    "        total_precip = dfp2['precip(cm/hr)'].sum()\n",
    "        print(\"\\nTotal precipitation (cm) : \" + str(total_precip))\n",
    "        df.loc[i,'Total precipitation (cm)'] = total_precip\n",
    "        \n",
    "    \n",
    "        # Total discharge \n",
    "        total_discharge =  dfq2['discharge(cm/hr)'].sum()\n",
    "        print(\"Total discharge (cm) : \" + str(total_discharge))\n",
    "        df.loc[i,'Total discharge (cm)'] = total_discharge\n",
    "        \n",
    "    \n",
    "        baseq,antQ_date,antQ_val,peakQ_date,peakQ,end_of_event,ending_discharge = hydrosep(dfq2)\n",
    "        \n",
    "        # Antecedent discharge \n",
    "        print(\"Antecedent discharge (cm/hr): \" + str(antQ_val))\n",
    "        df.loc[i,'Antecedent discharge (cm/hr)'] = antQ_val\n",
    "        \n",
    "        # Maximum precipitation \n",
    "        max_precip = dfp['precip(cm/hr)'].max()\n",
    "        print(\"Maximum precipitation intensity (cm/hr): \" + str(max_precip))\n",
    "        df.loc[i,'Maximum precipitation intensity (cm/hr)'] = max_precip\n",
    "        \n",
    "        # Peak event discharge \n",
    "        print(\"Peak event discharge (cm/hr): \" + str(peakQ))\n",
    "        print(\"Time of peak event: \" + str(peakQ_date))\n",
    "        df.loc[i,'Peak event discharge (cm/hr)'] = peakQ\n",
    "   \n",
    "        #  Seconds column in dfp2 \n",
    "        dfp2['seconds'] = dfp2.index.view('int64')/1e9\n",
    "        \n",
    "        #  Centroid of precipitation event \n",
    "        centroid = (dfp2['precip(cm/hr)']*dfp2['seconds']).sum()/(dfp2['precip(cm/hr)'].sum())\n",
    "        centroid = dt.datetime.fromtimestamp(centroid)\n",
    "        print(\"Centroid: \" + str(centroid))\n",
    "        \n",
    "        # Lag time \n",
    "        lagtime = peakQ_date-centroid\n",
    "        day_remainder = (lagtime.seconds)/86400\n",
    "        lagtime = lagtime.days + day_remainder\n",
    "        print(\"Lag time (days): \" + str(lagtime))\n",
    "        df.loc[i,'Centroid lag to peak (days)'] = lagtime\n",
    "        \n",
    "\n",
    "    \n",
    "    return (total_precip, total_discharge,baseq,antQ_date,antQ_val,peakQ_date,\n",
    "            peakQ,end_of_event,ending_discharge,\n",
    "            centroid,lagtime,df)\n",
    "print(color.BOLD +\"NH Regulated Storms  \"+ color.END)\n",
    "analyzestorm(dfstorm, dfp, dfq) \n",
    "print(color.BOLD +\"NH Unregulated Storms \"+ color.END)\n",
    "analyzestorm(dfstorm2, dfp, dfq3) \n",
    "print(color.BOLD +\"SC Unregulated Storms \"+ color.END)\n",
    "analyzestorm(dfstorm3, dfp4, dfq5) \n",
    "print(color.BOLD +\"SC Regulated Storms \"+ color.END)\n",
    "analyzestorm(dfstorm4, dfp4, dfq6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcb1683",
   "metadata": {},
   "source": [
    "### Plotting centroid lag to peak\n",
    "\n",
    "Two bar graphs were created from the previously calculated centroid lag to peak times for each of the regulated and unregulated storms. Each storm event was plotted adjacent to one another to compare the differences in centroid lag to peak times. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab2c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Centroid lag to peak NH\n",
    "\n",
    "X = np.arange(1)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.set_ylabel('Lag time  (days)')\n",
    "ax.set_xlabel('Storms ')\n",
    "ax.set_title('NH Regulated and Unregulated centroid lag to peak ')\n",
    "ax.bar(X + 0.00, dfstorm.iloc[0,-1], color = 'k', width = 0.1, label='Regulated')\n",
    "ax.bar(X + 0.1, dfstorm2.iloc[0,-1], color = 'r', width = 0.1, label= 'Unregulated')\n",
    "ax.bar(X + 0.4, dfstorm.iloc[1,-1], color = 'k', width = 0.1)\n",
    "ax.bar(X + .5, dfstorm2.iloc[1,-1], color = 'r', width = 0.1)\n",
    "ax.bar(X + 0.9, dfstorm.iloc[2,-1], color = 'k', width = 0.1)\n",
    "ax.bar(X + 1.0, dfstorm2.iloc[2,-1], color = 'r', width = 0.1)\n",
    "\n",
    "ax.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324d1f4",
   "metadata": {},
   "source": [
    "Figure 6. New Hampshire regulated and unregulated precipitation centroid lag to peak time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f02ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Centroid lag to peak SC\n",
    "\n",
    "X = np.arange(1)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.set_ylabel('Lag time  (days)')\n",
    "ax.set_xlabel('Storms ')\n",
    "ax.set_title('SC Regulated and Unregulated centroid lag to peak ')\n",
    "ax.bar(X + 0.00, dfstorm4.iloc[0,-1], color = 'k', width = 0.1, label='Regulated')\n",
    "ax.bar(X + 0.1, dfstorm3.iloc[0,-1], color = 'r', width = 0.1, label= 'Unregulated')\n",
    "ax.bar(X + 0.4, dfstorm4.iloc[1,-1], color = 'k', width = 0.1)\n",
    "ax.bar(X + .5, dfstorm3.iloc[1,-1], color = 'r', width = 0.1)\n",
    "ax.bar(X + 0.9, dfstorm4.iloc[2,-1], color = 'k', width = 0.1)\n",
    "ax.bar(X + 1.0, dfstorm3.iloc[2,-1], color = 'r', width = 0.1)\n",
    "\n",
    "ax.legend(loc='best')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc04bcbb",
   "metadata": {},
   "source": [
    "Figure 7. South Carolina regulated and unregulated precipitation centroid lag to peak time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884519b6",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The results of this study emphasize the influence that river regulation has on flood frequency. Regardless of land use type, hydrologic soil group or mean basin slope, rivers that had regulation had smaller peak annual discharge (figure3). \n",
    "\n",
    "\n",
    "Hydrologic soil groups differed drastically between the New Hampshire watersheds and the South Carolina watersheds, the soil types were very similar between each state's corresponding regulated and unregulated sites. It was surprising to see the NH regulated site have significantly smaller floods then all of the other sites despite having a relatively high concentration of slow and very slow infiltrating soil groups (figure2). In regards to land use most of the watersheds had similar landuse to their corresponding regulated or unregulated site.\n",
    "\n",
    "\n",
    "The New Hampshire watersheds are located in very rural areas containing vast forests which make up more than 80% of each  watershed's land use . However the South Carolina sites were located in more urban areas, with the Reedy River watershed comprising 38% urban areas and only 40% forest (figure1). Although the SC sites tended to be in more urban areas and the NH sites in more rural areas, the Reedy River site did have over 20% more urban areas then the regulated Saluda river, which possibly could have led to the increase in peak annual discharges. \n",
    "\n",
    "\n",
    "Mean basin slope was also derived for each of the watersheds to see if it influenced flood frequency. Whereas mean basin slope size didn’t seem to influence the magnitude and frequency of floods, it did seem to affect centroid lag to peak times. It has long been known that watershed slope and length are the two biggest factors in lag to peak times.(Askew 1970). Both the NH regulated site and South Carolina unregulated sites had significantly higher slopes then their counterpart.These large differences in mean basin slopes are most likely the reason the centroid lag to peak times are longer for the regulated NH site and the unregulated SC site (figure 7)\n",
    "\n",
    "\n",
    "While there are many other factors that contribute to high peak discharges in rivers that were analyzed such as soil type,land use classification and basin average slope,it was evident that regulation influenced flood size. The regulation of both the Connecticut River and Saluda Rivers has led to a substantial decrease in flood size and flood frequency (figure3 ).The correlation  between flood size and regulation was further explored through plotting both regulated and unregulated discharges.All six storms that were plotted had higher unregulated discharges then regulated discharges( figure 6). A number of studies have also found that dams on average reduce peak annual discharge by up to 90% and decrease the daily discharges by up to 60% (Graf 2006).While regulated sites had less discharge then unregulated sites, the study could be improved by creating a more specific metric then “regulated or not regulated”, such as number of dams or even as specific as reservoir storage capacity. Previous studies using reservoir storage capacity as a metric instead of regulation found that as water storage increases mean annual peak discharges decrease(Asquith 2001). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06af139",
   "metadata": {},
   "source": [
    "### Conclusion: \n",
    "\n",
    "This study investigated how regulation in rivers affect flood frequency ,total discharge between storm events and centroid lag to peak times. Floods regulated with dams such as the NH site experience a greater centroid to lag time potentially due to water being slowed by the dams along the Connecticut River. However the mean basin slope seemed to be more linked to the centroid lag  to peak times across both the NH and SC sites.  All six of the regulated storms analyzed had smaller total discharges and lower peak discharges then their corresponding unregulated gauge. The regulated annual peak discharges being lower than the unregulated peak discharges across all exceedance probabilities reinforce the idea that regulation helps absorb small flood events and significantly reduce larger events. Despite the peak annual discharges at the regulated site being less than the unregulated sites across all exceedance probabilities,there are still many more factors to consider when comparing across these diverse groups of rivers. Due to the relatively small sample size of four watersheds, more data is needed in order to draw any solid conclusions from this study. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541987fc",
   "metadata": {},
   "source": [
    "### Literature Cited \n",
    "\n",
    "Asquith, William. (2001). Effects of Regulation on L-Moments of Annual Peak Streamflow in Texas.\n",
    "\n",
    "Askew, A.J. 1970(b). Variation in lag time for natural catchments. Journal of the\n",
    "Hydraulics Division. Proceedings of the American Society of Civil Engineers.\n",
    "Vol. 96, HY2, pp. 317-329.\n",
    "\n",
    "Ayalew, T. B., Krajewski, W. F., and Mantilla, R. (2013). “Exploring the effect of reservoir storage on peak discharge frequency.” J. Hydrol.Eng., 10.1061/(ASCE)HE.1943-5584.0000721, 1697–1708.\n",
    "\n",
    "Rood, Stewart & Goater, Lori & Mahoney, John & Pearce, Cheryl & Smith, Derald. (2007). Floods, fire, and ice: disturbance ecology of riparian cottonwoods. Canadian Journal of Botany. 85. 1019-1032. 10.1139/B07-073.\n",
    "\n",
    "William L. Graf,Downstream hydrologic and geomorphic effects of large dams on American rivers,Geomorphology,Volume 79, Issues 3–4,2006,Pages 336-360,ISSN 0169-555X\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
